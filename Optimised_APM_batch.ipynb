{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>optAPM</h1>\n",
    "\n",
    "Optimized Absolute Plate Motion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pygplates as pgp\n",
    "import pmagpy.ipmag as ipmag\n",
    "import pmagpy.pmag as pmag\n",
    "import geoTools\n",
    "import nlopt\n",
    "import ipyparallel\n",
    "\n",
    "from optapm import ModelSetup as ms, ProcessResults as pr\n",
    "from functools import partial\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch parallel client\n",
    "try:\n",
    "\n",
    "    rc = ipyparallel.Client(profile='default')\n",
    "    print \"Cores started: \", len(rc.ids)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print \"\"\n",
    "    print \"! Caught exception: \", e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Set model parameters, load data, and calculate starting conditions</h3>\n",
    "\n",
    "Sets all user-selected parameters for the mode run\n",
    "\n",
    "<b>Arguments:</b>\n",
    "\n",
    "* geographical_uncertainty : Number that approximately represents geographical uncertainty - 95% confidence limit around \n",
    "  ref pole location \n",
    "* rotation_uncertainty : Number that represents the upper and lower bounds of the optimisation's angle variation\n",
    "* sample_space : Selects the sampling method to be used to generate start seeds. \"Fisher\" (spherical distribution) by default.\n",
    "* models : The total number of models to be produced. 1 model = 1 complete optimisation from 1 starting location\n",
    "* model_stop_condition : Type of condition to be used to terminate optimisation. \"threshold\" or \"max_iter\". Threshold by default.\n",
    "* max_iter : IF \"max_iter\" selected, sets maximum iterations regardless of successful convergence.\n",
    "* ref_rotation_plate_id : Plate to be used as fixed reference. 701 (Africa) by default.\n",
    "* ref_rotation_start_age : Rotation begin age.\n",
    "* ref_rotation_end_age : Rotation end age.\n",
    "* interpolation_resolution : Resolution in degrees used in the Fracture Zone calulcations\n",
    "* rotation_age_of_interest : The result we are interested in. Allows for 'windowed mean' approach. It is the midpoint between the start and end ages by default.\n",
    "\t\t\t\n",
    "<b>Data to be included in optimisation: True by default.</b>\n",
    "\n",
    "* fracture_zones : Boolean\n",
    "* net_rotation : Boolean\n",
    "* trench_migration : Boolean\n",
    "* hotspot_reconstruction : Boolean\n",
    "* hotspot_dispersion : Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Toggle single or batch model run\n",
    "batch = False\n",
    "\n",
    "# model_name = \"optAPM144\"\n",
    "# start_age = 10\n",
    "end_age = 0\n",
    "interval = 1\n",
    "\n",
    "search = \"Initial\"\n",
    "search_radius = 60\n",
    "rotation_uncertainty = 30\n",
    "auto_calc_ref_pole = True\n",
    "models = 2\n",
    "\n",
    "# model_stop_condition = 'threshold'\n",
    "#model_stop_condition = 'max_iter'  # Generally used for NR calculations as they do not converge\n",
    "max_iter = 250\n",
    "\n",
    "# fracture_zones   = False\n",
    "# net_rotation     = True\n",
    "# trench_migration = True\n",
    "# hotspot_trails   = False\n",
    "\n",
    "# # sigma (i.e. cost / sigma = weight)\n",
    "# fracture_zone_weight    = 1.\n",
    "# net_rotation_weight     = 1.\n",
    "# trench_migration_weight = 1.\n",
    "# hotspot_trails_weight   = 1.\n",
    "\n",
    "\n",
    "# Trench migration parameters\n",
    "tm_method = 'pygplates' # 'pygplates' for new method OR 'convergence' for old method\n",
    "tm_data_type = 'muller2016' # 'muller2016' or 'shephard2013'\n",
    "\n",
    "\n",
    "# Hotspot parameters:\n",
    "interpolated_hotspot_trails = True\n",
    "use_trail_age_uncertainty = True\n",
    "\n",
    "# Millions of years - e.g. 2 million years @ 50mm per year = 100km radius uncertainty ellipse\n",
    "trail_age_uncertainty_ellipse = 1\n",
    "\n",
    "include_chains = ['Louisville', 'Tristan', 'Reunion', 'St_Helena', 'Foundation', 'Cobb', 'Samoa', 'Tasmantid', \n",
    "                  'Hawaii']\n",
    "\n",
    "#include_chains = ['Louisville', 'Tristan', 'Reunion', 'Hawaii', 'St_Helena', 'Tasmantid']\n",
    "\n",
    "\n",
    "# # Rotation file with existing APM rotations removed from 0-230Ma to be used:\n",
    "# rotfile = 'Global_EarthByte_230-0Ma_GK07_AREPS_' + model_name + '.rot'\n",
    "# #rotfile = 'Shephard_etal_ESR2013_Global_EarthByte_2013_' + model_name + '.rot'\n",
    "\n",
    "# print \"Rotation file to be used: \", rotfile\n",
    "# print \"TM data:\", tm_data_type\n",
    "# print \"TM method:\", tm_method\n",
    "# print \"Age range for model:\", np.arange(end_age + interval, start_age + interval, interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start timer\n",
    "#main_start = time.time()\n",
    "\n",
    "min_results = []\n",
    "mean_results = []\n",
    "\n",
    "\n",
    "print \"Cores started: \", len(rc.ids)\n",
    "\n",
    "dview = rc[:]\n",
    "dview.block = True\n",
    "\n",
    "costs = []\n",
    "\n",
    "if batch == True:\n",
    "    \n",
    "    pass\n",
    "    \n",
    "#print \"Batch model run\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Generate model set arrays [FZ, NR, TM, HS]\n",
    "\n",
    "# model_name_set = [ \n",
    "#                    'optAPM158', 'optAPM159', 'optAPM160', 'optAPM161',\n",
    "#                    'optAPM162', 'optAPM163', 'optAPM164', 'optAPM165',\n",
    "#                    'optAPM166'\n",
    "#                  ]\n",
    "\n",
    "# start_age_set = [80, 80, 80, 80, 220, 220, 220, 220, 220]\n",
    "\n",
    "# model_constraint_set = [ \n",
    "#                          [False, True, True, True],   # NR(1), TM(1), HS(1)   -  optAPM1\n",
    "#                          [False, True, True, True],   # NR(1), TM(1), HS(0.5) -  optAPM2\n",
    "#                          [False, True, True, True],   # NR(1), TM(0.5), HS(1) -  optAPM3\n",
    "#                          [False, True, True, True],   # NR(0.5), TM(1), HS(1) -  optAPM4\n",
    "#                          [False, True, True, False],  # NR(1), TM(1)          -  optAPM5\n",
    "#                          [False, True, True, False],  # NR(1), TM(0.5)        -  optAPM6\n",
    "#                          [False, True, True, False],  # NR(0.5), TM(1)        -  optAPM7\n",
    "#                          [False, False, True, False], # TM(1)                 -  optAPM8\n",
    "#                          [False, True, False, False]  # NR(1)                 -  optAPM9\n",
    "#                        ] \n",
    "\n",
    "# model_weighting_set = [ \n",
    "#                         [1., 1., 1., 1.],\n",
    "#                         [1., 1., 1., 0.5],\n",
    "#                         [1., 1., 0.5, 1.],\n",
    "#                         [1., 0.5, 1., 1.],\n",
    "#                         [1., 1., 1., 1.],\n",
    "#                         [1., 1., 0.5, 1.],\n",
    "#                         [1., 0.5, 1., 1.],\n",
    "#                         [1., 1., 1., 1.],\n",
    "#                         [1., 1., 1., 1.]\n",
    "#                       ]\n",
    "\n",
    "# model_stop_condition_set = [ \n",
    "#                              'threshold',\n",
    "#                              'threshold',\n",
    "#                              'threshold',\n",
    "#                              'threshold',\n",
    "#                              'threshold',\n",
    "#                              'threshold',\n",
    "#                              'threshold',\n",
    "#                              'threshold',\n",
    "#                              'max_iter'\n",
    "#                            ]\n",
    "\n",
    "# # Loop through all models in model constraint set\n",
    "# for ii in xrange(0, len(model_name_set)):\n",
    "\n",
    "#     model_name = model_name_set[ii]\n",
    "\n",
    "#     start_age = start_age_set[ii]\n",
    "\n",
    "#     fracture_zones   = model_constraint_set[ii][0]\n",
    "#     net_rotation     = model_constraint_set[ii][1]\n",
    "#     trench_migration = model_constraint_set[ii][2]\n",
    "#     hotspot_trails   = model_constraint_set[ii][3]\n",
    "\n",
    "#     # sigma (i.e. cost / sigma = weight)\n",
    "#     fracture_zone_weight    = model_weighting_set[ii][0]\n",
    "#     net_rotation_weight     = model_weighting_set[ii][1]\n",
    "#     trench_migration_weight = model_weighting_set[ii][2]\n",
    "#     hotspot_trails_weight   = model_weighting_set[ii][3]\n",
    "\n",
    "#     model_stop_condition = model_stop_condition_set[ii]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "    \n",
    "elif batch == False:\n",
    "    \n",
    "    print \"Single model run\"\n",
    "\n",
    "    model_name = \"optAPM175\"\n",
    "    start_age = 80\n",
    "    model_stop_condition = 'threshold'\n",
    "    max_iter = 5\n",
    "    interval = 10\n",
    "\n",
    "    fracture_zones   = False\n",
    "    net_rotation     = True\n",
    "    trench_migration = True\n",
    "    hotspot_trails   = False\n",
    "\n",
    "    # sigma (i.e. cost / sigma = weight)\n",
    "    fracture_zone_weight    = 1.\n",
    "    net_rotation_weight     = 1.\n",
    "    trench_migration_weight = 1.\n",
    "    hotspot_trails_weight   = 1.\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    age_range = np.arange(end_age + interval, start_age + interval, interval)\n",
    "\n",
    "    # Rotation file with existing APM rotations removed from 0-230Ma to be used:\n",
    "    if tm_data_type == 'muller2016':\n",
    "        \n",
    "        rotfile = 'Global_EarthByte_230-0Ma_GK07_AREPS_' + model_name + '.rot'\n",
    "        \n",
    "    elif tm_data_type == 'shephard2013':\n",
    "        \n",
    "        rotfile = 'Shephard_etal_ESR2013_Global_EarthByte_2013_' + model_name + '.rot'\n",
    "\n",
    "    print \"Rotation file to be used: \", rotfile\n",
    "    print \"TM data:\", tm_data_type\n",
    "    print \"TM method:\", tm_method\n",
    "    print \"Age range for model:\", np.arange(end_age + interval, start_age + interval, interval)\n",
    "\n",
    "\n",
    "    # Loop through all times\n",
    "    for i in xrange(0, len(age_range)):\n",
    "\n",
    "    #     if fracture_zones == True:\n",
    "\n",
    "    #         if age_range[i] <= 40:\n",
    "\n",
    "    #             fracture_zones = True\n",
    "\n",
    "    #         else:\n",
    "\n",
    "    #             fracture_zones = False\n",
    "\n",
    "        print \"-------------------------------------------------------------------\"\n",
    "        print \"\"\n",
    "        print model_name\n",
    "        print \"\"\n",
    "\n",
    "        # Large area grid search to find minima\n",
    "        if search == 'Initial':\n",
    "\n",
    "            search_type = 'Random'\n",
    "\n",
    "        # Uses grid search minima as seed for targeted secondary search (optional)\n",
    "        elif search == 'Secondary':\n",
    "\n",
    "            search_type = 'Uniform'\n",
    "            search_radius = 15\n",
    "            rotation_uncertainty = 30\n",
    "\n",
    "            models = 60\n",
    "            auto_calc_ref_pole = False\n",
    "\n",
    "        ref_rot_longitude = -53.5\n",
    "        ref_rot_latitude = 56.6\n",
    "        ref_rot_angle = -2.28\n",
    "\n",
    "        ref_rotation_plate_id = 701\n",
    "        ref_rotation_start_age = age_range[i]\n",
    "        ref_rotation_end_age = ref_rotation_start_age - interval\n",
    "        #ref_rotation_end_age = 0.\n",
    "\n",
    "        interpolation_resolution = 5\n",
    "        rotation_age_of_interest = True\n",
    "\n",
    "        datadir = '/Users/Simon/GIT/optAPM/data/'\n",
    "    #     datadir = '/Users/Mike/Projects/optAPM/data/'\n",
    "        pmag_rotfile = 'Palaeomagnetic_Africa_S.rot'\n",
    "\n",
    "        if tm_method == 'convergence':\n",
    "\n",
    "            if tm_data_type == 'muller2016':\n",
    "\n",
    "                nnr_datadir = 'TMData/'\n",
    "                nnr_rotfile = 'Global_EarthByte_230-0Ma_GK07_AREPS_NNR.rot'\n",
    "\n",
    "\n",
    "        elif tm_method == 'pygplates':\n",
    "\n",
    "            if tm_data_type == 'muller2016':\n",
    "\n",
    "                nnr_datadir = 'TMData/Muller_2016/'\n",
    "                nnr_rotfile = 'Global_EarthByte_230-0Ma_GK07_AREPS_NNR.rot'\n",
    "\n",
    "            elif tm_data_type == 'shephard2013':\n",
    "\n",
    "                nnr_datadir = 'TMData/Shephard_2013/'\n",
    "                nnr_rotfile = 'Shephard_etal_ESR2013_Global_EarthByte_NNR_ORIGINAL.rot'\n",
    "\n",
    "        ridge_file = 'Global_EarthByte_230-0Ma_GK07_AREPS_Ridges.gpml'\n",
    "        isochron_file = 'Global_EarthByte_230-0Ma_GK07_AREPS_Isochrons.gpmlz'\n",
    "        isocob_file = 'Global_EarthByte_230-0Ma_GK07_AREPS_IsoCOB.gpml'\n",
    "        hst_file = 'HotspotTrails.geojson'\n",
    "        hs_file = 'HotspotCatalogue2.geojson'\n",
    "        interpolated_hotspots = 'interpolated_hotspot_chains_5Myr.xlsx'\n",
    "\n",
    "\n",
    "        print \"Start age:\", ref_rotation_start_age, \"Ma\"\n",
    "        print \"\"\n",
    "        print \"Search type:\", search\n",
    "        print \"Search radius:\", search_radius\n",
    "        print \"\"\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        # Gather parameters\n",
    "        params = [search_radius, rotation_uncertainty, search_type, models, model_stop_condition, max_iter,\n",
    "                  ref_rotation_plate_id, ref_rotation_start_age, ref_rotation_end_age, interpolation_resolution, \n",
    "                  rotation_age_of_interest, fracture_zones, net_rotation, trench_migration, hotspot_trails,\n",
    "                  ref_rot_longitude, ref_rot_latitude, ref_rot_angle, auto_calc_ref_pole, search, \n",
    "                  fracture_zone_weight, net_rotation_weight, trench_migration_weight, hotspot_trails_weight,\n",
    "                  include_chains, interpolated_hotspot_trails, tm_method]\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        # Load all data\n",
    "        data = ms.dataLoader(datadir, rotfile, pmag_rotfile, nnr_rotfile=nnr_rotfile, nnr_datadir=nnr_datadir, \n",
    "                             ridge_file=ridge_file, isochron_file=isochron_file, isocob_file=isocob_file, \n",
    "                             hst_file=hst_file, hs_file=hs_file, interpolated_hotspots=interpolated_hotspots)\n",
    "\n",
    "\n",
    "        # Calculate starting conditions\n",
    "        startingConditions = ms.modelStartConditions(params, data)\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # --------------------------------------------------------------------\n",
    "        # Objective function\n",
    "\n",
    "        def obj_f(x, grad):\n",
    "\n",
    "            import numpy as np\n",
    "            import pygplates as pgp\n",
    "            import optimisation_methods\n",
    "            import obj_func_convergence\n",
    "            import geoTools\n",
    "            import pmagpy.pmag as pmag\n",
    "            import subduction_convergence_for_absolute_plate_motion as scap\n",
    "\n",
    "            from optapm import ObjectiveFunctions\n",
    "            from scipy import stats\n",
    "\n",
    "            print \"Data array\", data_array\n",
    "\n",
    "\n",
    "            # Prepare rotation model for updates during optimisation - keeps rotations in memory\n",
    "            file_registry = pgp.FeatureCollectionFileFormatRegistry()\n",
    "            rotation_model_tmp = file_registry.read(rotation_file)\n",
    "\n",
    "\n",
    "\n",
    "            #### -----------------------------------------------------------------------------------------\n",
    "            #### 1. Calculate reconstructed data point locations\n",
    "\n",
    "            tmp_opt_rlon = []\n",
    "            tmp_opt_rlat = []\n",
    "            opt_stats = []\n",
    "\n",
    "            import geoTools\n",
    "            # Check incoming Africa finite rotation pole values\n",
    "            lat_, lon_ = geoTools.checkLatLon(x[1], x[0])\n",
    "            ang_ = x[2]\n",
    "\n",
    "\n",
    "            #### -----------------------------------------------------------------------------------------\n",
    "            #### 2. Find and update Africa rotation\n",
    "\n",
    "\n",
    "            # Find existing rotation for Africa at correct time\n",
    "            opt_rotation_feature = None\n",
    "            for rotation_feature in rotation_model_tmp:\n",
    "\n",
    "                total_reconstruction_pole = rotation_feature.get_total_reconstruction_pole()\n",
    "\n",
    "                if total_reconstruction_pole:\n",
    "\n",
    "                    fixed_plate_id, moving_plate_id, rotation_sequence = total_reconstruction_pole\n",
    "\n",
    "                    if fixed_plate_id == 001 and moving_plate_id == 701:\n",
    "\n",
    "                        opt_rotation_feature = rotation_feature\n",
    "                        break\n",
    "\n",
    "\n",
    "            # Update rotation file with proposed Africa rotation\n",
    "            if opt_rotation_feature:\n",
    "\n",
    "                adjustment_time = pgp.GeoTimeInstant(ref_rotation_start_age)\n",
    "\n",
    "                for finite_rotation_samples in rotation_sequence.get_enabled_time_samples():\n",
    "\n",
    "                    finite_rotation_time = finite_rotation_samples.get_time()\n",
    "\n",
    "                    if finite_rotation_time == ref_rotation_start_age:\n",
    "\n",
    "                        finite_rotation = finite_rotation_samples.get_value().get_finite_rotation()\n",
    "\n",
    "                        new_rotation = pgp.FiniteRotation((np.double(lat_), np.double(lon_)), \n",
    "                                                          np.radians(np.double(ang_)))\n",
    "                        finite_rotation_samples.get_value().set_finite_rotation(new_rotation)\n",
    "\n",
    "            rotation_model_updated = pgp.RotationModel(rotation_model_tmp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #### -----------------------------------------------------------------------------------------\n",
    "            #### 3. Calculate data fits\n",
    "\n",
    "\n",
    "            #\n",
    "            # Fracture zone orientation\n",
    "            if data_array[0] == True:\n",
    "\n",
    "                # Get skew values\n",
    "                fz = optimisation_methods.Calc_Median(rotation_model_updated, PID, \n",
    "                                                      seafloor_ages, Lats, Lons, \n",
    "                                                      spreading_directions)\n",
    "\n",
    "\n",
    "                tmp_fz_eval = fz[0] + fz[1]\n",
    "\n",
    "                fz_eval = tmp_fz_eval / fracture_zone_weight\n",
    "\n",
    "\n",
    "\n",
    "            #\n",
    "            # Net rotation\n",
    "            if data_array[1] == True:\n",
    "\n",
    "                # Prepare no net rotation model for updates during optimisation - keeps rotations in memory\n",
    "                nnr_file_registry = file_registry.read(no_net_rotation_file)\n",
    "                nn_rotation_model = pgp.RotationModel(nnr_file_registry)\n",
    "\n",
    "                nr_timesteps = np.arange(ref_rotation_end_age, ref_rotation_start_age + 1, 2)\n",
    "\n",
    "                PTLong1, PTLat1, PTangle1, SPLong, SPLat, SPangle, SPLong_NNR, SPLat_NNR, SPangle_NNR = \\\n",
    "                optimisation_methods.ApproximateNR_from_features(rotation_model_updated, nn_rotation_model, \n",
    "                                                                 nr_timesteps, ref_rotation_plate_id)\n",
    "\n",
    "                tmp_nr_eval = (np.sum(np.abs(PTangle1)) + np.mean(np.abs(PTangle1))) / 2\n",
    "\n",
    "                nr_eval = tmp_nr_eval / net_rotation_weight\n",
    "\n",
    "\n",
    "\n",
    "            #\n",
    "            # Trench migration\n",
    "\n",
    "            # Old method\n",
    "            if data_array[2] == True and tm_method == 'convergence':\n",
    "\n",
    "                kinArray = obj_func_convergence.kinloop(ref_rotation_end_age, ref_rotation_start_age, reformArray, \n",
    "                                                        rotation_model_tmp)\n",
    "\n",
    "                cA = obj_func_convergence.kinstats(kinArray)\n",
    "                cA = np.array(cA)\n",
    "\n",
    "                trench_vel = -cA[:,6]\n",
    "                trench_vel_SD = np.std(trench_vel)\n",
    "                trench_numRetreating = len(np.where(trench_vel > 0)[0])\n",
    "                trench_numAdvancing = len(trench_vel) - trench_numRetreating\n",
    "                trench_numOver30 = len(np.where(trench_vel > 30)[0])\n",
    "                trench_numLessNeg30 = len(np.where(trench_vel < -30)[0])\n",
    "                trench_numTotal = len(trench_vel)\n",
    "                trench_sumAbsVel_n = np.sum(np.abs(trench_vel)) / len(trench_vel)\n",
    "\n",
    "                trench_percent_retreat = round((np.float(trench_numRetreating) / np.float(trench_numTotal)) * 100, 2)\n",
    "                trench_percent_advance = 100. - trench_percent_retreat\n",
    "\n",
    "                # Calculate cost\n",
    "                #tm_eval_1 = (trench_percent_advance * 10) / trench_migration_weight\n",
    "                #tm_eval_2 = (trench_sumAbsVel_n * 15) / trench_migration_weight\n",
    "\n",
    "                # 1. trench percent advance + trench abs vel mean\n",
    "                #tm_eval = (tm_eval_1 + tm_eval_2) / 2\n",
    "\n",
    "                # 2. trench_abs_vel_mean\n",
    "                #tm_eval_2 = (np.sum(np.abs(trench_vel)) / len(trench_vel)) / trench_migration_weight\n",
    "\n",
    "                # 3. number of trenches in advance\n",
    "                #tm_eval_3 = (trench_numAdvancing * 2) / trench_migration_weight\n",
    "\n",
    "                # 4. abs median\n",
    "                #tm_eval_4 = np.median(abs(trench_vel)) / trench_migration_weight\n",
    "\n",
    "                # 5. standard deviation\n",
    "                #tm_eval_5 = np.std(trench_vel) / trench_migration_weight\n",
    "\n",
    "                # 6. variance\n",
    "                #tm_stats = stats.describe(trench_vel)\n",
    "                #tm_eval = tm_stats.variance / trench_migration_weight\n",
    "\n",
    "                # 7. trench absolute motion abs vel mean\n",
    "                #tm_eval_7 = ((np.sum(np.abs(trench_vel)) / len(trench_vel)) * 15) / trench_migration_weight\n",
    "\n",
    "                #tm_eval = tm_eval_5\n",
    "\n",
    "\n",
    "\n",
    "                #---- old ones\n",
    "                # Minimise trench advance\n",
    "    #             tm_eval_1 = ((trench_percent_advance * 10) / trench_migration_weight)**2\n",
    "                #tm_eval_1 = (trench_percent_advance * 10) / trench_migration_weight\n",
    "\n",
    "                # Minimise trench velocities\n",
    "    #             tm_eval_2 = ((trench_sumAbsVel_n * 15) / trench_migration_weight)**2\n",
    "                #tm_eval_2 = (trench_sumAbsVel_n * 15) / trench_migration_weight\n",
    "\n",
    "                # Minimise trenches moving very fast (< or > 30)\n",
    "                #tm_eval_3 = (trench_numOver30 + trench_numLessNeg30) * trench_migration_weight\n",
    "\n",
    "    #             # V1 (Original)\n",
    "    #             tmp_tm_eval = ((trench_vel_SD * (trench_numRetreating * trench_sumAbsVel_n)) / \\\n",
    "    #                            (trench_numTotal - (trench_numOver30 + trench_numLessNeg30)))\n",
    "\n",
    "    #             tm_eval = tmp_tm_eval * trench_migration_weight\n",
    "\n",
    "\n",
    "\n",
    "            # New method\n",
    "            elif data_array[2] == True and tm_method == 'pygplates':\n",
    "\n",
    "                tm_data = pgp.FeatureCollection(nnr_datadir + 'TMData_%sMa.gpml' % (int(ref_rotation_start_age)))\n",
    "\n",
    "                # Calculate trench segment stats\n",
    "                interval = 10.\n",
    "                tm_stats = scap.subduction_absolute_motion(rotation_model_updated,\n",
    "                                                           tm_data,\n",
    "                                                           np.radians(1.),\n",
    "                                                           ref_rotation_start_age - interval)\n",
    "\n",
    "                # Process tm_stats to extract values for use in cost function\n",
    "                trench_vel = []\n",
    "                trench_obl = []\n",
    "\n",
    "                for i in xrange(0, len(tm_stats)):\n",
    "\n",
    "                    trench_vel.append(tm_stats[i][2])\n",
    "                    trench_obl.append(tm_stats[i][3])\n",
    "\n",
    "                trench_vel = np.array(trench_vel)\n",
    "                trench_obl = np.array(trench_obl)\n",
    "\n",
    "                # Scale velocities from cm to mm\n",
    "                trench_vel = trench_vel * 10\n",
    "\n",
    "                # Calculate trench orthogonal velocity\n",
    "                tm_vel_orth = np.abs(trench_vel) * -np.cos(np.radians(trench_obl)) \n",
    "\n",
    "\n",
    "                trench_numTotal = len(tm_vel_orth)\n",
    "                trench_numRetreating = len(np.where(tm_vel_orth > 0)[0])\n",
    "                trench_numAdvancing = len(tm_vel_orth) - trench_numRetreating\n",
    "                trench_percent_retreat = round((np.float(trench_numRetreating) / np.float(trench_numTotal)) * 100, 2)\n",
    "                trench_percent_advance = 100. - trench_percent_retreat\n",
    "                trench_sumAbsVel_n = np.sum(np.abs(tm_vel_orth)) / len(tm_vel_orth)\n",
    "                trench_numOver30 = len(np.where(tm_vel_orth > 30)[0])\n",
    "                trench_numLessNeg30 = len(np.where(tm_vel_orth < -30)[0])\n",
    "\n",
    "                # Calculate cost\n",
    "                #tm_eval_1 = (trench_percent_advance * 10) / trench_migration_weight\n",
    "                #tm_eval_2 = (trench_sumAbsVel_n * 15) / trench_migration_weight\n",
    "\n",
    "                # 1. trench percent advance + trench abs vel mean\n",
    "                #tm_eval = (tm_eval_1 + tm_eval_2) / 2\n",
    "\n",
    "                # 2. trench_abs_vel_mean orthogonal\n",
    "                tm_eval_2 = (np.sum(np.abs(tm_vel_orth)) / len(tm_vel_orth)) / trench_migration_weight\n",
    "\n",
    "                # 3. number of trenches in advance\n",
    "                #tm_eval_3 = (trench_numAdvancing * 2) / trench_migration_weight\n",
    "\n",
    "                # 4. abs median\n",
    "                #tm_eval = np.median(abs(np.array(tm_vel_orth))) / trench_migration_weight\n",
    "\n",
    "                # 5. standard deviation\n",
    "                tm_eval_5 = (np.std(tm_vel_orth) / trench_migration_weight)\n",
    "\n",
    "                # 6. variance\n",
    "                #tm_stats = stats.describe(tm_vel_orth)\n",
    "                #tm_eval_6 = tm_stats.variance / trench_migration_weight\n",
    "\n",
    "                # 7. trench absolute motion abs vel mean\n",
    "                #tm_eval_7 = ((np.sum(np.abs(trench_vel)) / len(trench_vel)) * 15) / trench_migration_weight\n",
    "\n",
    "                tm_eval = ((tm_eval_2 + tm_eval_5) * 3) / trench_migration_weight\n",
    "                \n",
    "                # Original equation\n",
    "                #tm_eval = ((tm_eval_5 * (trench_numRetreating * trench_sumAbsVel_n)) / \\\n",
    "                #           (trench_numTotal - (trench_numOver30 + trench_numLessNeg30)))\n",
    "                \n",
    "                #tm_eval = ((tm_eval_2 + tm_eval_5 + trench_numAdvancing) / trench_numRetreating) / trench_migration_weight\n",
    "\n",
    "\n",
    "\n",
    "            # Hotspot trail distance misfit\n",
    "            if data_array[3] == True:\n",
    "\n",
    "                # returns: [point_distance_misfit, trail_distance_misfit, uncertainty, trail_name]\n",
    "                hs = ObjectiveFunctions.hotspot_trail_misfit(trail_data, ref_rotation_start_age, \n",
    "                                                             rotation_model_updated, use_trail_age_uncertainty,\n",
    "                                                             trail_age_uncertainty_ellipse)\n",
    "\n",
    "                if use_trail_age_uncertainty == False:\n",
    "\n",
    "                    tmp_distance_median = np.median(hs[0])\n",
    "                    tmp_distance_sd = np.std(hs[0])\n",
    "\n",
    "                    hs_dist_eval = (tmp_distance_median + tmp_distance_sd) / hotspot_trails_weight\n",
    "\n",
    "\n",
    "                elif use_trail_age_uncertainty == True:\n",
    "\n",
    "                    weighted_dist = []\n",
    "\n",
    "                    # Positively weight modelled distances that are less than uncertainty limit\n",
    "                    for i in xrange(0, len(hs[0])):\n",
    "\n",
    "                        if hs[0][i] < hs[2][i]:\n",
    "\n",
    "                            weighted_dist.append(hs[0][i] / 2)\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            weighted_dist.append(hs[0][i] * 2)\n",
    "\n",
    "\n",
    "                    tmp_distance_median = np.median(weighted_dist)\n",
    "                    tmp_distance_sd = np.std(weighted_dist)\n",
    "\n",
    "                    hs_dist_eval = (tmp_distance_median + tmp_distance_sd) / hotspot_trails_weight\n",
    "                    #hs_dist_eval = tmp_distance_median / hotspot_trails_weight\n",
    "                    #hs_dist_eval = tmp_distance_sd / hotspot_trails_weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #### -----------------------------------------------------------------------------------------\n",
    "            #### 3. Calculate evaluation return number\n",
    "\n",
    "            # Scaling values\n",
    "            alpha = 10\n",
    "            beta = 100\n",
    "            gamma = 1000\n",
    "\n",
    "\n",
    "            opt_eval = 0\n",
    "\n",
    "            # Fracture zones\n",
    "            try:\n",
    "                if fz_eval:\n",
    "                    opt_eval = opt_eval + (fz_eval * alpha)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # Net rotation\n",
    "            try:\n",
    "                if nr_eval:\n",
    "                    opt_eval = opt_eval + (nr_eval * gamma)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # Trench migration\n",
    "            try:\n",
    "                if tm_eval:\n",
    "                    #opt_eval = opt_eval + (tm_eval / alpha)\n",
    "                    opt_eval = opt_eval + tm_eval\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # Hotspot reconstruction distance + spherical dispersion statistics\n",
    "            try:\n",
    "                if hs_dist_eval and data_array[3] == True:\n",
    "\n",
    "                    # Distance only\n",
    "                    #opt_eval = opt_eval + hs_dist_eval\n",
    "\n",
    "                    # Kappa only\n",
    "                    #opt_eval = opt_eval + (hs_kappa_eval * 1e6)\n",
    "\n",
    "                    # Distance + Kappa\n",
    "                    #opt_eval = opt_eval + (((hs_kappa_eval * 1e6) + hs_dist_eval) / 1.5)\n",
    "\n",
    "                    # Distance misfit\n",
    "                    opt_eval = opt_eval + (hs_dist_eval / 2)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #### ---------------------------------------------------------------------------------------------\n",
    "            #### Return all calculated quantities     \n",
    "            try:\n",
    "                opt_eval_data.append(opt_eval)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            return opt_eval\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # --------------------------------------------------------------------\n",
    "        # Function to run optimisation routine\n",
    "\n",
    "        def run_optimisation(x, opt_n, N, obj_f, lb, ub, model_stop_condition, max_iter, rotation_file, \n",
    "                             no_net_rotation_file, ref_rotation_start_age, Lats, Lons, spreading_directions, \n",
    "                             spreading_asymmetries, seafloor_ages, PID, CPID, data_array, nnr_datadir, \n",
    "                             ref_rotation_end_age, ref_rotation_plate_id, reformArray, trail_data,\n",
    "                             fracture_zone_weight, net_rotation_weight, trench_migration_weight, hotspot_trails_weight,\n",
    "                             use_trail_age_uncertainty, trail_age_uncertainty_ellipse, tm_method):\n",
    "\n",
    "            import nlopt\n",
    "\n",
    "            print tm_method\n",
    "\n",
    "            opt = nlopt.opt(nlopt.LN_COBYLA, opt_n)\n",
    "            opt.set_min_objective(obj_f)\n",
    "            opt.set_lower_bounds(lb)\n",
    "            opt.set_upper_bounds(ub)\n",
    "\n",
    "            # Select model stop condition\n",
    "            if model_stop_condition != 'threshold':\n",
    "\n",
    "                opt.set_maxeval(max_iter)\n",
    "\n",
    "            else:\n",
    "\n",
    "                opt.set_ftol_rel(1e-6)\n",
    "                opt.set_xtol_rel(1e-8)\n",
    "\n",
    "            xopt = opt.optimize(x)\n",
    "            minf = opt.last_optimum_value()    \n",
    "\n",
    "            return xopt, minf\n",
    "\n",
    "\n",
    "        # Map variables for use locally and by number of cores selected\n",
    "        run_optimisation = dview['run_optimisation'] = run_optimisation\n",
    "        opt_n = dview['opt_n'] = startingConditions[1]\n",
    "        N = dview['N'] = startingConditions[2]\n",
    "        obj_f = dview['obj_f'] = obj_f\n",
    "        lb = dview['lb'] = startingConditions[3]\n",
    "        ub = dview['ub'] = startingConditions[4]\n",
    "        model_stop_condition = dview['model_stop_condition'] = startingConditions[5]\n",
    "        max_iter = dview['max_iter'] = startingConditions[6]\n",
    "        rotation_file = dview['rotation_file'] = data[1]\n",
    "        ref_rotation_start_age = dview['ref_rotation_start_age'] = startingConditions[8]\n",
    "        ref_rotation_end_age = dview['ref_rotation_end_age'] = startingConditions[9]\n",
    "        ref_rotation_plate_id = dview['ref_rotation_plate_id'] = startingConditions[10]\n",
    "        Lats = dview['Lats'] = startingConditions[11]\n",
    "        Lons = dview['Lons'] = startingConditions[12]\n",
    "        spreading_directions = dview['spreading_directions'] = startingConditions[13]\n",
    "        spreading_asymmetries = dview['spreading_asymmetries'] = startingConditions[14]\n",
    "        seafloor_ages = dview['seafloor_ages'] = startingConditions[15]\n",
    "        PID = dview['PID'] = startingConditions[16]\n",
    "        CPID = dview['CPID'] = startingConditions[17]\n",
    "        data_array = dview['data_array'] = startingConditions[18]\n",
    "        nnr_datadir = dview['nnr_datadir'] = startingConditions[19]\n",
    "        no_net_rotation_file = dview['no_net_rotation_file'] = startingConditions[20]\n",
    "        reformArray = dview['reformArray'] = startingConditions[21]\n",
    "        trail_data = dview['trail_data'] = startingConditions[22]\n",
    "\n",
    "        dview['fracture_zone_weight'] = fracture_zone_weight\n",
    "        dview['net_rotation_weight'] = net_rotation_weight\n",
    "        dview['trench_migration_weight'] = trench_migration_weight\n",
    "        dview['hotspot_trails_weight'] = hotspot_trails_weight\n",
    "        dview['use_trail_age_uncertainty'] = use_trail_age_uncertainty\n",
    "        dview['trail_age_uncertainty_ellipse'] = trail_age_uncertainty_ellipse\n",
    "        dview['tm_method'] = tm_method\n",
    "\n",
    "        start_seeds = startingConditions[23]\n",
    "        rotation_age_of_interest_age = startingConditions[24]\n",
    "        data_array_labels_short = startingConditions[25]\n",
    "\n",
    "        if auto_calc_ref_pole == True:\n",
    "\n",
    "            ref_rot_longitude = startingConditions[26]\n",
    "            ref_rot_latitude = startingConditions[27]\n",
    "            ref_rot_angle = startingConditions[28]\n",
    "\n",
    "        elif auto_calc_ref_pole == False:\n",
    "\n",
    "            ref_rot_longitude = ref_rot_longitude\n",
    "            ref_rot_latitude = ref_rot_latitude\n",
    "            ref_rot_angle = ref_rot_angle\n",
    "\n",
    "        seed_lons = startingConditions[29]\n",
    "        seed_lats = startingConditions[30]\n",
    "\n",
    "        x = startingConditions[0]\n",
    "\n",
    "        minf = []\n",
    "\n",
    "        #print \"Number of start seeds generated:\", len(start_seeds)\n",
    "        print \"Optimised models to be run:\", len(start_seeds)\n",
    "        print \" \"\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # --------------------------------------------------------------------\n",
    "        # Start optimisation\n",
    "\n",
    "        # Start timer\n",
    "        #start = time.time()\n",
    "        main_start = time.time()\n",
    "\n",
    "        # Run optimisation algorithm in parallel\n",
    "        #try:\n",
    "\n",
    "        prunopt = partial(run_optimisation, opt_n=opt_n, N=N, obj_f=obj_f, lb=lb, ub=ub, \n",
    "                          model_stop_condition=model_stop_condition, max_iter=max_iter, rotation_file=rotation_file,\n",
    "                          no_net_rotation_file=no_net_rotation_file, ref_rotation_start_age=ref_rotation_start_age, \n",
    "                          Lats=Lats, Lons=Lons, spreading_directions=spreading_directions, \n",
    "                          spreading_asymmetries=spreading_asymmetries, \n",
    "                          seafloor_ages=seafloor_ages, PID=PID, CPID=CPID, data_array=data_array, nnr_datadir=nnr_datadir,\n",
    "                          ref_rotation_end_age=ref_rotation_end_age, ref_rotation_plate_id=ref_rotation_plate_id,\n",
    "                          reformArray=reformArray, trail_data=trail_data, fracture_zone_weight=fracture_zone_weight,\n",
    "                          net_rotation_weight=net_rotation_weight, trench_migration_weight=trench_migration_weight,\n",
    "                          hotspot_trails_weight=hotspot_trails_weight, use_trail_age_uncertainty=use_trail_age_uncertainty,\n",
    "                          trail_age_uncertainty_ellipse=trail_age_uncertainty_ellipse, tm_method=tm_method)\n",
    "\n",
    "        xopt = dview.map(prunopt, x)\n",
    "\n",
    "\n",
    "    #     except Exception as e:\n",
    "\n",
    "    #         text_file = open(\"Output.txt\", \"w\")\n",
    "    #         text_file.write(\"Model error: \" + str(e))\n",
    "    #         text_file.close()\n",
    "\n",
    "\n",
    "        # Find minimum result from all models\n",
    "        results = []\n",
    "\n",
    "        for i in xrange(0, len(xopt)):\n",
    "\n",
    "            results.append(xopt[i][1])\n",
    "\n",
    "        min_result_index = np.where(results == np.min(results))[0][0]\n",
    "        min_result = xopt[min_result_index]\n",
    "\n",
    "        print \" \"\n",
    "        print \"Optimisation complete.\"\n",
    "        print \"Models produced:\", len(xopt)\n",
    "        print \" \"\n",
    "\n",
    "\n",
    "        # Save results to pickle file located as '/model_output/\n",
    "        output_file = pr.saveResultsToPickle(data_array, data_array_labels_short, ref_rotation_start_age, \n",
    "                                             ref_rotation_end_age, search_radius, xopt, models, model_name)\n",
    "\n",
    "\n",
    "        # Plot results\n",
    "        rmin, rmean = pr.sortAndPlot(output_file, ref_rotation_start_age, ref_rotation_end_age, \n",
    "                                     rotation_age_of_interest_age, xopt, rotation_file, ref_rot_longitude,\n",
    "                                     ref_rot_latitude, ref_rot_angle, seed_lons, seed_lats, \n",
    "                                     ref_rotation_plate_id, model_name, models, data_array_labels_short, \n",
    "                                     data_array, search_radius)\n",
    "\n",
    "\n",
    "        for j in xrange(0, len(xopt)):\n",
    "\n",
    "            costs.append(xopt[j][1])\n",
    "\n",
    "\n",
    "        rmin = np.array(rmin)\n",
    "        rmean = np.array(rmean)\n",
    "\n",
    "        min_results.append(rmin[0])\n",
    "        mean_results.append(rmean[0])\n",
    "\n",
    "        import geoTools\n",
    "        plat, plon = geoTools.checkLatLon(min_results[-1][2], min_results[-1][3])\n",
    "\n",
    "\n",
    "    #     end_time = round(time.time() - start, 2)\n",
    "    #     sec = timedelta(seconds = float(end_time))\n",
    "    #     dt = datetime(1,1,1) + sec\n",
    "\n",
    "    #     print \"Timestep completed in:\"\n",
    "    #     print str(dt.day-1) + \"d, \" + str(dt.hour) + \"h, \" + str(dt.minute) + \"m, \" + str(dt.second) + \"s.\"\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # --------------------------------------------------------------------\n",
    "        # Update rotation file with result\n",
    "\n",
    "        #rotation_file = datadir + 'Global_EarthByte_230-0Ma_GK07_AREPS_optAPM003.rot'\n",
    "        rotation_file = datadir + rotfile\n",
    "\n",
    "        file_registry1 = pgp.FeatureCollectionFileFormatRegistry()\n",
    "        rotation_model_tmp = file_registry1.read(rotation_file)\n",
    "\n",
    "        # Find existing rotations for Africa\n",
    "        opt_rotation_feature = None\n",
    "        for rotation_feature in rotation_model_tmp:\n",
    "\n",
    "            total_reconstruction_pole = rotation_feature.get_total_reconstruction_pole()\n",
    "\n",
    "            if total_reconstruction_pole:\n",
    "\n",
    "                fixed_plate_id, moving_plate_id, rotation_sequence = total_reconstruction_pole\n",
    "\n",
    "                if fixed_plate_id == 001 and moving_plate_id == 701:\n",
    "                    opt_rotation_feature = rotation_feature\n",
    "                    break\n",
    "\n",
    "\n",
    "        # Update existing rotation in the model with result\n",
    "        if opt_rotation_feature:\n",
    "\n",
    "            adjustment_time = pgp.GeoTimeInstant(ref_rotation_start_age)\n",
    "\n",
    "            for finite_rotation_samples in rotation_sequence.get_enabled_time_samples():\n",
    "\n",
    "                finite_rotation_time = finite_rotation_samples.get_time()\n",
    "\n",
    "                if finite_rotation_time == ref_rotation_start_age:\n",
    "\n",
    "                    finite_rotation = finite_rotation_samples.get_value().get_finite_rotation()\n",
    "\n",
    "    #                 new_rotation = pgp.FiniteRotation((np.double(round(min_results[-1][2], 2)), \n",
    "    #                                                    np.double(round(min_results[-1][3], 2))), \n",
    "    #                                                    np.radians(np.double(round(min_results[-1][1], 2))))\n",
    "\n",
    "                    new_rotation = pgp.FiniteRotation((np.double(round(plat, 2)), \n",
    "                                                       np.double(round(plon, 2))), \n",
    "                                                       np.radians(np.double(round(min_results[-1][1], 2))))\n",
    "\n",
    "                    finite_rotation_samples.get_value().set_finite_rotation(new_rotation)\n",
    "\n",
    "\n",
    "        # Add result rotation pole to rotation file\n",
    "        tmp_file_registry = pgp.FeatureCollectionFileFormatRegistry()\n",
    "        tmp_file_registry.write(rotation_model_tmp, rotation_file)\n",
    "\n",
    "\n",
    "    main_end_time = round(time.time() - main_start, 10)\n",
    "    main_sec = timedelta(seconds = float(main_end_time))\n",
    "    main_dt = datetime(1,1,1) + main_sec\n",
    "\n",
    "    print \"\"\n",
    "    print \"\"\n",
    "    print \"Model completed in:\"\n",
    "    print str(main_dt.day-1) + \"d, \" + str(main_dt.hour) + \"h, \" + str(main_dt.minute) + \"m, \" + str(main_dt.second) + \"s.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling (mean of 0-50Ma - 20 models)\n",
    "\n",
    "* NR: 7:574, 3:465\n",
    "\n",
    "* TM: 334\n",
    "\n",
    "* HS: 398"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display result arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.mean(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Mean of 20 models (0-50Ma)\"\n",
    "print \"\"\n",
    "print \"tm_eval =\", 47 * 3\n",
    "print \"nr_eval =\", 143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_output/optAPM175_10-0Ma_2models_NR_TM_60.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "print data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
